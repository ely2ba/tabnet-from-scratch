
# TabNet from Scratch (In Progress)

This project is a full PyTorch reimplementation of **TabNet**, a deep learning architecture proposed by Google Research in 2019. TabNet introduces a novel approach to modeling tabular data using **sequential attention**, **sparse feature selection**, and **interpretable decision steps**.

## 🔍 Objectives

- Reconstruct TabNet architecture from the ground up (Feature Transformer, Attentive Transformer, Sparsemax)
- Apply model to real-world tabular datasets (e.g. Adult Income)
- Benchmark performance against traditional models like XGBoost and LightGBM
- Visualize learned attention masks and feature importances across decision steps

## 📁 Project Structure (coming soon)

- `notebooks/`: Training & evaluation workflows
- `src/`: Model implementation
- `utils/`: Preprocessing and data loading
- `visuals/`: Attention mask visualizations
- `README.md`: This document

## 🧠 Status

🟡 **Implementation in progress**  
Architecture design and module testing underway. Initial results expected in April 2025.



