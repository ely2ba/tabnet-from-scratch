# TabNet from Scratch (In Progress)

This project is a full PyTorch reimplementation of TabNet, a deep learning architecture for tabular data introduced by Google Research (2019). TabNet leverages sequential attention and sparse feature selection to learn interpretable, high-performing representations from structured inputs.

---

## ğŸ” Objectives

- Reconstruct the full TabNet architecture from first principles, including:
  - Feature Transformer
  - Attentive Transformer
  - Sparsemax activation
- Apply the model to real-world tabular datasets (e.g., Adult Income)
- Benchmark against traditional models such as XGBoost and LightGBM
- Visualize learned attention masks and feature importances across decision steps

---

## ğŸ› ï¸ Features (Planned)

- Modular PyTorch implementation of:
  - Sparsemax activation
  - Feature and Attentive Transformers
  - Multi-step sequential attention flow
- Benchmarking utilities and training scripts
- Visual tools for exploring:
  - Feature masks at each decision step
  - Attention progression and model interpretability

---

## ğŸ“ Project Structure (coming soon)

```
tabnet-from-scratch/
â”œâ”€â”€ src/               Core model implementation
â”œâ”€â”€ notebooks/         Training and evaluation workflows
â”œâ”€â”€ utils/             Preprocessing and data loading
â”œâ”€â”€ visuals/           Attention mask visualizations
â””â”€â”€ README.md          This document
```

---

## ğŸš§ Status

ğŸŸ¡ Implementation in progress.  
âœ”ï¸ Sparsemax implemented and tested. Core architecture components in progress.
Initial experiments and benchmarking expected in April 2025.

---

## ğŸ“œ License

MIT
