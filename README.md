
# TabNet from Scratch (In Progress)

This project is a full PyTorch reimplementation of **TabNet**, a deep learning architecture proposed by Google Research in 2019. TabNet introduces a novel approach to modeling tabular data using **sequential attention**, **sparse feature selection**, and **interpretable decision steps**.

## ğŸ” Objectives

- Reconstruct TabNet architecture from the ground up (Feature Transformer, Attentive Transformer, Sparsemax)
- Apply model to real-world tabular datasets (e.g. Adult Income)
- Benchmark performance against traditional models like XGBoost and LightGBM
- Visualize learned attention masks and feature importances across decision steps

## ğŸ“ Project Structure (coming soon)

- `notebooks/`: Training & evaluation workflows
- `src/`: Model implementation
- `utils/`: Preprocessing and data loading
- `visuals/`: Attention mask visualizations
- `README.md`: This document

## ğŸ§  Status

ğŸŸ¡ **Implementation in progress**  
Architecture design and module testing underway. Initial results expected in April 2025.



